{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Lab 4: Attribute Transformation an Dimensionality Reduction"]},{"cell_type":"markdown","metadata":{"id":"EHYqQxpQ6hJE"},"source":["## Attribute Transformation\n","\n","An attribute transform is a function that maps the entire set of values of a given attribute to a new set of replacement values such that each old value can be identified with one of the new values\n","\n","Dataset contains features with different metrics and scales. For example: \"pregnant\" and \"insulin\" values are based on different scales of measurement. The magnitude of \"insulin\" value is higher than \"pregnant\" in the diabetes dataset. Hence, many algorithm that are sensitive to varying scales of value will be biased towards the one with higher magnitdue.For example neural netwroks are highly sensitive to scaling of the data attributes. Hence we need to convert the dataset into suitable format before it is fed into the neurons.\n","\n","#### The solution to varying scale values\n","\n","We need a mechanism that scales all the attribute values into a given range typically between 0 to +1 or between a certain specified range. This approach is called feature scaling.\n","\n","Below are two approaches taht converts each feature into same scale\n","\n","1. Min-Max Scaler (Normalization)\n","2. Standardization\n"]},{"cell_type":"markdown","metadata":{"id":"gVKKhofF6hJH"},"source":["## Using MinMaxScaler() \n","\n","Rescaling X_train dataset\n","\n","\"minj\" and \"maxj\" represent the minimum and maximum values of attribute 'j'. The $j^{th}$ attribute value $x_{i}^{j}$ of the $i^{th}$ row is scaled as:\n","\n","### $y_{i}^{j} = (x_{i}^{j} - min_{j})/(max_{j}-min_{j})$\n","\n","We transform only the train dataset for scaling or any data tranformation tasks."]},{"cell_type":"markdown","metadata":{"id":"zvFre-Sl6hJI"},"source":["#### Split the cleaned data into input features $(X_{i})$ and output component (Y)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"gFpc8Jhe6hJI","outputId":"af7d4371-18f0-4efe-92d7-c33a58b93137"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aLImNdEt6hJK","outputId":"3f2eab97-5463-464d-ef79-9491ee1da9e4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>pregnant</th>\n","      <th>glucose</th>\n","      <th>bp</th>\n","      <th>skin</th>\n","      <th>insulin</th>\n","      <th>bmi</th>\n","      <th>pedigree</th>\n","      <th>age</th>\n","      <th>Diabetic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>85.0</td>\n","      <td>66.0</td>\n","      <td>29.000000</td>\n","      <td>125.0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>183.0</td>\n","      <td>64.0</td>\n","      <td>29.142593</td>\n","      <td>125.0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>89.0</td>\n","      <td>66.0</td>\n","      <td>23.000000</td>\n","      <td>94.0</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>137.0</td>\n","      <td>40.0</td>\n","      <td>35.000000</td>\n","      <td>168.0</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>116.0</td>\n","      <td>74.0</td>\n","      <td>29.142593</td>\n","      <td>125.0</td>\n","      <td>25.6</td>\n","      <td>0.201</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  pregnant  glucose    bp       skin  insulin   bmi  pedigree  \\\n","0           0         1     85.0  66.0  29.000000    125.0  26.6     0.351   \n","1           1         8    183.0  64.0  29.142593    125.0  23.3     0.672   \n","2           2         1     89.0  66.0  23.000000     94.0  28.1     0.167   \n","3           3         0    137.0  40.0  35.000000    168.0  43.1     2.288   \n","4           4         5    116.0  74.0  29.142593    125.0  25.6     0.201   \n","\n","   age  Diabetic  \n","0   31         0  \n","1   32         1  \n","2   21         0  \n","3   33         1  \n","4   30         0  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dbts_new= pd.read_csv('imputed_data_diabetes.csv')\n","dbts_new.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"f5rEkKty6hJL"},"outputs":[],"source":["spltd_data = dbts_new.values\n","# separate the dataset into input and output components\n","X = spltd_data [:,0:8]\n","Y = spltd_data[:,8]"]},{"cell_type":"markdown","metadata":{"id":"hZ4FvhEG6hJL"},"source":["#### Separate the splitted dataset into training and testing dataset with training  dataset = 80% of cleaned data and test dataset  = 20% of cleaned dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rllrcT-g6hJL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"]},{"cell_type":"markdown","metadata":{"id":"nr0zlfX66hJM"},"source":["#### Use Sci-Kit learn MinMaxScaler () for normlization"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"s7eYwvRH6hJM","outputId":"e6d3af3f-f575-4f36-cc47-da8108ec7270"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.8629 0.4706 0.7935 0.8367 0.4239 0.2608 0.3967 0.0371]\n"," [0.8081 0.1176 0.4387 0.6327 0.3804 0.1755 0.4131 0.0717]\n"," [0.6906 0.1176 0.5032 0.3673 0.1196 0.1106 0.2372 0.2728]\n"," [0.0875 0.0588 0.329  0.4286 0.0652 0.0288 0.0286 0.1093]\n"," [0.6488 0.4118 0.9742 0.4694 0.2826 0.1575 0.1411 0.0363]]\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","sclr = MinMaxScaler(feature_range=(0, 1))\n","scaled_data_X_train = sclr.fit_transform(X_train)\n","# summarize transformed data\n","np.set_printoptions(precision=4)\n","print(scaled_data_X_train[0:5,:])"]},{"cell_type":"markdown","metadata":{"id":"wacBLwM76hJM"},"source":["#### The above code converted all the feature values into the  scale between 0 and 1 using Normalization or Min-Max scaling.\n","<font color = green>Some learning algorithms like Neural Networks expect input values between [0,1] hence we use normalization for scaling in such case. </font>"]},{"cell_type":"markdown","metadata":{"id":"wnHqmie06hJN"},"source":["#### Standardization\n","\n","It is another approach to scaling where the scaled value isn't within the [0,1] range. It is suitable where the data collection process has errors and hence has extreme values or outliers.\n","\n","The $j^{th}$ attribute value $x_{i}^{j}$ of the ith row is normalized by:\n","\n","###                         Z-score_normalization (x')=  ($x_{i}^{j}$ -$\\mu_{j}$)  /  $\\sigma_{j}$\n","\n"," where the $j^{th}$  attribute has mean $\\mu_{j}$ and standard deviation $\\sigma_{j}$ .\n","                       \n","We use a function \"StandardScaler()\"  for standardization purpose."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jrri4zbh6hJN","outputId":"eb57189b-833c-4b91-edf0-c560625f89fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 1.259  1.257  1.482  2.722  1.852  1.076  0.706 -0.918]\n"," [ 1.07  -0.542 -0.323  1.104  1.402  0.222  0.82  -0.677]\n"," [ 0.665 -0.542  0.005 -0.999 -1.302 -0.428 -0.409  0.721]\n"," [-1.414 -0.842 -0.881 -0.514 -1.866 -1.245 -1.868 -0.416]\n"," [ 0.521  0.957  2.401 -0.19   0.388  0.042 -1.081 -0.924]]\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","scale_ftrs_stndrd = StandardScaler().fit(X_train)\n","scaled_stndrd_X_train = scale_ftrs_stndrd.transform(X_train)\n","# summarize transformed data\n","np.set_printoptions(precision=3)\n","print(scaled_stndrd_X_train[0:5,:])"]},{"cell_type":"markdown","metadata":{"id":"r4ZcdKI46hJN"},"source":["## Dimensionality Reduction\n","\n","Dimensionality reduction is all about summarizing the data with most of the information preserved in compact form.Reducing the dimension of the feature space, creates fewer relationships between variables and hence the model is less likely to overfit.\n","\n","#### One of such technique discussed here is the Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality-reduction technique for reducing the dimensionality of large data sets, i.e. by transforming a large set of input features into a smaller set which still contains most of the information in the original dataset .But Before applying PCA, the dataset must be rescaled, if not rescaled, the model/algorithm's accuracy may not be improved much."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Dtnw70x56hJO","outputId":"b7cbe93a-9631-4da3-e6f4-0eabdd090270"},"outputs":[{"name":"stdout","output_type":"stream","text":["Explained Variance: [0.259 0.159 0.142]\n"]}],"source":["from sklearn.decomposition import PCA\n","\n","prcpl_cmpnts = PCA(n_components=3)  # use three diagonal compnents for data reduction and summarization\n","prncpl_smmry = prcpl_cmpnts.fit(scaled_stndrd_X_train)\n","print((\"Explained Variance: %s\") % (prncpl_smmry.explained_variance_ratio_)) # summarize the components\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9UXzIyb06hJO","outputId":"3fd89790-5b6f-4e47-e679-96b7f5922849"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.032  0.107  0.445  0.35   0.48   0.337  0.521  0.227]\n"," [ 0.066  0.495 -0.191  0.493  0.142 -0.471  0.102 -0.467]\n"," [-0.051  0.512  0.473  0.108 -0.405  0.396 -0.395 -0.153]]\n"]}],"source":["print(prncpl_smmry.components_)"]},{"cell_type":"markdown","metadata":{"id":"KciK_0Wd6hJO"},"source":["\n","Above code created three principial components as denoted in three separate arrays. Each array represents the component that summarizes the overall data."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
